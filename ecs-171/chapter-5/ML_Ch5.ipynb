{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Ch5.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1J2QCHlIwRXRkYOZJSC_iPvWQbIwaGg_W","authorship_tag":"ABX9TyNLPw7LJq5GbpcxRTn7gk4R"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DIUMsG1KNqQC","colab_type":"text"},"source":["# Chapter 5: Tweaking the Trees: Bagging, Random Forests, and Boosting"]},{"cell_type":"markdown","metadata":{"id":"ougnN5_3Gu2J","colab_type":"text"},"source":["* ***ensemble methods*** can be used to combine predictions of several base estimators in order to make a better predictions (more robust or more generalizable)\n","\n","  * could be multiple types of algorithms\n","\n","\n","\n","* some include combining trees in *bagging, boosting, RF*\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f9NipcZuN573","colab_type":"text"},"source":["## 5.1. Bias vs. Variance, Bagging and Boosting"]},{"cell_type":"markdown","metadata":{"id":"vZnPq8dtiOgZ","colab_type":"text"},"source":["  * **bagging/averaging/RF** methods reduce variance\n","\n","      * trees may overlap but reduces effect of noise\n","\n","  * **boosting** methods reduce bias"]},{"cell_type":"markdown","metadata":{"id":"0GqXBX7cxtUa","colab_type":"text"},"source":["## Bagging: Generating New Trees by Resampling"]},{"cell_type":"markdown","metadata":{"id":"c2hztjZTOASK","colab_type":"text"},"source":["* DT can be sensitive to sampling variation\n","\n","  * eg. a few samples could change the entire tree structure\n","\n","  * one thing we could do is generate more trees when the sample size is too small so as to reduce the variance"]},{"cell_type":"markdown","metadata":{"id":"l9CMLiC7xyJ0","colab_type":"text"},"source":["* Bagging method: using bootstrap method (from stats) on DTs\n","\n","  * random sample **m** points from **n** data points (*with replacement*) to get **s** new samples\n","\n","  * sample size **s**, and **m** are going to be are hyperparameters here\n","\n","  * reduces variance; reduces overfitting"]},{"cell_type":"markdown","metadata":{"id":"9Ob1mLsr5-gv","colab_type":"text"},"source":["### 5.2.1. The Method"]},{"cell_type":"markdown","metadata":{"id":"rKNiMMIn6BgR","colab_type":"text"},"source":["* aggregate **s** trees; combine predicted values to form final predicition \n","\n","  * eg. *average* values in regression \n","\n","  * *voting* or averaging votes among trees before choosing the max probability in classification \n","\n","   * `scikit-learn` implementation combines classifiers by averaging their probabilistic prediction (not voting)\n","\n","* ***random forests*** is the specific version of the implememntation where we place a limit on the number of features considered in each split\n","\n","  * reduces the correlation between trees reducing variance\n","\n","    * since often the same features are chosen every time, otherwise\n","\n","* ***ExtraTreesClassifier and ExtraTreesRegressor*** \n","\n","  * like RF but also with best of randomized split thresholds instead of best splits of all (adds even more randomness)\n","\n","  * decreasing variance and increases bias even more\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ab4wtf5eG7hv","colab_type":"text"},"source":["### 5.2.2. Example: Vertebrae Data"]},{"cell_type":"code","metadata":{"id":"xF7UtdreOqtJ","colab_type":"code","outputId":"8ec3bee6-6730-4185-823e-aa6ae25b718f","executionInfo":{"status":"ok","timestamp":1589148403431,"user_tz":420,"elapsed":1286,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import pandas as pd\n","my_path = '/content/drive/My Drive/ecs171_yancey/Lecture_Notes/Chapter_2/column_3C.dat'\n","vert = pd.read_csv(my_path, sep=' ',header=None)\n","\n","# note, that data is separated into groups of class (eg. all DH come first)\n","vert.head()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>63.03</td>\n","      <td>22.55</td>\n","      <td>39.61</td>\n","      <td>40.48</td>\n","      <td>98.67</td>\n","      <td>-0.25</td>\n","      <td>DH</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>39.06</td>\n","      <td>10.06</td>\n","      <td>25.02</td>\n","      <td>29.00</td>\n","      <td>114.41</td>\n","      <td>4.56</td>\n","      <td>DH</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>68.83</td>\n","      <td>22.22</td>\n","      <td>50.09</td>\n","      <td>46.61</td>\n","      <td>105.99</td>\n","      <td>-3.53</td>\n","      <td>DH</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>69.30</td>\n","      <td>24.65</td>\n","      <td>44.31</td>\n","      <td>44.64</td>\n","      <td>101.87</td>\n","      <td>11.21</td>\n","      <td>DH</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>49.71</td>\n","      <td>9.65</td>\n","      <td>28.32</td>\n","      <td>40.06</td>\n","      <td>108.17</td>\n","      <td>7.92</td>\n","      <td>DH</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       0      1      2      3       4      5   6\n","0  63.03  22.55  39.61  40.48   98.67  -0.25  DH\n","1  39.06  10.06  25.02  29.00  114.41   4.56  DH\n","2  68.83  22.22  50.09  46.61  105.99  -3.53  DH\n","3  69.30  24.65  44.31  44.64  101.87  11.21  DH\n","4  49.71   9.65  28.32  40.06  108.17   7.92  DH"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Uh6-Cm-QG_sc","colab_type":"text"},"source":["* `sklearn.ensemble` package has RF module"]},{"cell_type":"markdown","metadata":{"id":"NIuhH-4s2fQ4","colab_type":"text"},"source":["* we use test set sizes of 100 and default hyper-parameters"]},{"cell_type":"code","metadata":{"id":"WRRbJwq2Qy3b","colab_type":"code","outputId":"ee401a8e-182f-405c-a865-f5f234d87976","executionInfo":{"status":"ok","timestamp":1589149350741,"user_tz":420,"elapsed":26117,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn import tree\n","from statistics import mean\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","nreps = 100\n","treeacc = list(range(0,100))\n","foracc = list(range(0,100))\n","etacc = list(range(0,100))\n","\n","for i in range(0,nreps):\n","  \n","  X_train, X_test, y_train, y_test = train_test_split(vert.iloc[:,0:5], vert.iloc[:,6], test_size=100)\n","\n","  clf = tree.DecisionTreeClassifier()\n","  clf = clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  treeacc[i] = accuracy_score(y_test, y_pred)\n","\n","  clf = RandomForestClassifier()\n","  clf = clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  foracc[i] = accuracy_score(y_test, y_pred)\n","\n","  clf = ExtraTreesClassifier()\n","  clf = clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  etacc[i] = accuracy_score(y_test, y_pred)\n","\n","print('The DT accuracy is:', mean(treeacc))\n","print('The RF accuracy is:', mean(foracc))\n","print('The ET accuracy is:', mean(etacc))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["The DT accuracy is: 0.6441\n","The RF accuracy is: 0.7174\n","The ET accuracy is: 0.7094\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3jVeQBKi6590","colab_type":"text"},"source":["* So we have an improvement using RF"]},{"cell_type":"markdown","metadata":{"id":"vLKQQqC56-gI","colab_type":"text"},"source":["### 5.2.3. Parallelization"]},{"cell_type":"markdown","metadata":{"id":"tsn9xOy0-u1K","colab_type":"text"},"source":["* parallel construction of the trees and predictions through the n_jobs parameter.\n","\n","* **n_jobs=k** then computations are partitioned into **k** jobs (run on k cores)\n","\n","  * **n_jobs=-1** then all cores available on the machine are used.\n","  \n","  * Due to the inter-process communication overhead this is mostly only helpful if building a large number of trees, or on large datasets (eg. large trees)."]},{"cell_type":"markdown","metadata":{"id":"dl9dxrrA_7fT","colab_type":"text"},"source":["#### 5.2.3.1. Ex. Remote-Sensing Soil Analysis"]},{"cell_type":"markdown","metadata":{"id":"gZ06xy15Ul5E","colab_type":"text"},"source":["From the website:\n","\n","> Advances in rapid, low cost analysis of soil samples using infrared spectroscopy, georeferencing of soil samples, and greater availability of earth remote sensing data provide new opportunities for predicting soil functional properties at unsampled locations. Soil functional properties are those properties related to a soil’s capacity to support essential ecosystem services such as primary productivity, nutrient and water retention, and resistance to soil erosion. Digital mapping of soil functional properties, especially in data sparse regions such as Africa, is important for planning sustainable agricultural intensification and natural resources management.\n","\n"]},{"cell_type":"code","metadata":{"id":"LXxH0-Ym65Qp","colab_type":"code","outputId":"208bb00e-55db-4445-fa5e-fe62a52db978","executionInfo":{"status":"ok","timestamp":1589221426867,"user_tz":420,"elapsed":4656,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":253}},"source":["import pandas as pd\n","my_path = '/content/drive/My Drive/ecs171_yancey/Lecture_Notes/Chapter_5/training.csv'\n","afrsoil = pd.read_csv(my_path)\n","\n","afrsoil.head()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PIDN</th>\n","      <th>m7497.96</th>\n","      <th>m7496.04</th>\n","      <th>m7494.11</th>\n","      <th>m7492.18</th>\n","      <th>m7490.25</th>\n","      <th>m7488.32</th>\n","      <th>m7486.39</th>\n","      <th>m7484.46</th>\n","      <th>m7482.54</th>\n","      <th>m7480.61</th>\n","      <th>m7478.68</th>\n","      <th>m7476.75</th>\n","      <th>m7474.82</th>\n","      <th>m7472.89</th>\n","      <th>m7470.97</th>\n","      <th>m7469.04</th>\n","      <th>m7467.11</th>\n","      <th>m7465.18</th>\n","      <th>m7463.25</th>\n","      <th>m7461.32</th>\n","      <th>m7459.39</th>\n","      <th>m7457.47</th>\n","      <th>m7455.54</th>\n","      <th>m7453.61</th>\n","      <th>m7451.68</th>\n","      <th>m7449.75</th>\n","      <th>m7447.82</th>\n","      <th>m7445.89</th>\n","      <th>m7443.97</th>\n","      <th>m7442.04</th>\n","      <th>m7440.11</th>\n","      <th>m7438.18</th>\n","      <th>m7436.25</th>\n","      <th>m7434.32</th>\n","      <th>m7432.4</th>\n","      <th>m7430.47</th>\n","      <th>m7428.54</th>\n","      <th>m7426.61</th>\n","      <th>m7424.68</th>\n","      <th>...</th>\n","      <th>m634.473</th>\n","      <th>m632.544</th>\n","      <th>m630.616</th>\n","      <th>m628.687</th>\n","      <th>m626.759</th>\n","      <th>m624.83</th>\n","      <th>m622.902</th>\n","      <th>m620.973</th>\n","      <th>m619.045</th>\n","      <th>m617.116</th>\n","      <th>m615.188</th>\n","      <th>m613.259</th>\n","      <th>m611.331</th>\n","      <th>m609.402</th>\n","      <th>m607.474</th>\n","      <th>m605.545</th>\n","      <th>m603.617</th>\n","      <th>m601.688</th>\n","      <th>m599.76</th>\n","      <th>BSAN</th>\n","      <th>BSAS</th>\n","      <th>BSAV</th>\n","      <th>CTI</th>\n","      <th>ELEV</th>\n","      <th>EVI</th>\n","      <th>LSTD</th>\n","      <th>LSTN</th>\n","      <th>REF1</th>\n","      <th>REF2</th>\n","      <th>REF3</th>\n","      <th>REF7</th>\n","      <th>RELI</th>\n","      <th>TMAP</th>\n","      <th>TMFI</th>\n","      <th>Depth</th>\n","      <th>Ca</th>\n","      <th>P</th>\n","      <th>pH</th>\n","      <th>SOC</th>\n","      <th>Sand</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>XNhoFZW5</td>\n","      <td>0.302553</td>\n","      <td>0.301137</td>\n","      <td>0.299748</td>\n","      <td>0.300354</td>\n","      <td>0.302679</td>\n","      <td>0.303799</td>\n","      <td>0.301702</td>\n","      <td>0.298936</td>\n","      <td>0.298126</td>\n","      <td>0.298120</td>\n","      <td>0.298163</td>\n","      <td>0.299124</td>\n","      <td>0.300828</td>\n","      <td>0.302522</td>\n","      <td>0.303633</td>\n","      <td>0.303364</td>\n","      <td>0.302018</td>\n","      <td>0.301226</td>\n","      <td>0.300803</td>\n","      <td>0.299270</td>\n","      <td>0.297354</td>\n","      <td>0.296703</td>\n","      <td>0.297569</td>\n","      <td>0.298991</td>\n","      <td>0.299680</td>\n","      <td>0.299230</td>\n","      <td>0.298567</td>\n","      <td>0.298865</td>\n","      <td>0.299278</td>\n","      <td>0.298186</td>\n","      <td>0.296781</td>\n","      <td>0.296565</td>\n","      <td>0.296190</td>\n","      <td>0.294805</td>\n","      <td>0.293779</td>\n","      <td>0.293980</td>\n","      <td>0.295162</td>\n","      <td>0.297448</td>\n","      <td>0.300198</td>\n","      <td>...</td>\n","      <td>1.91489</td>\n","      <td>1.91967</td>\n","      <td>1.91974</td>\n","      <td>1.91909</td>\n","      <td>1.92077</td>\n","      <td>1.91855</td>\n","      <td>1.90573</td>\n","      <td>1.88994</td>\n","      <td>1.87770</td>\n","      <td>1.86431</td>\n","      <td>1.84816</td>\n","      <td>1.83288</td>\n","      <td>1.81858</td>\n","      <td>1.80247</td>\n","      <td>1.78462</td>\n","      <td>1.76644</td>\n","      <td>1.75086</td>\n","      <td>1.74335</td>\n","      <td>1.74246</td>\n","      <td>-0.630435</td>\n","      <td>-0.700000</td>\n","      <td>-0.783875</td>\n","      <td>-0.364146</td>\n","      <td>1.165479</td>\n","      <td>1.062682</td>\n","      <td>-0.716713</td>\n","      <td>-0.090016</td>\n","      <td>-0.861091</td>\n","      <td>-0.537106</td>\n","      <td>-0.722567</td>\n","      <td>-0.646673</td>\n","      <td>1.687734</td>\n","      <td>0.190708</td>\n","      <td>0.056843</td>\n","      <td>Topsoil</td>\n","      <td>-0.295749</td>\n","      <td>-0.041336</td>\n","      <td>-1.129366</td>\n","      <td>0.353258</td>\n","      <td>1.269748</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9XNspFTd</td>\n","      <td>0.270192</td>\n","      <td>0.268555</td>\n","      <td>0.266964</td>\n","      <td>0.267938</td>\n","      <td>0.271013</td>\n","      <td>0.272346</td>\n","      <td>0.269870</td>\n","      <td>0.266976</td>\n","      <td>0.266544</td>\n","      <td>0.266766</td>\n","      <td>0.266464</td>\n","      <td>0.266817</td>\n","      <td>0.268150</td>\n","      <td>0.269933</td>\n","      <td>0.271409</td>\n","      <td>0.271396</td>\n","      <td>0.270126</td>\n","      <td>0.269351</td>\n","      <td>0.268984</td>\n","      <td>0.267680</td>\n","      <td>0.265901</td>\n","      <td>0.265088</td>\n","      <td>0.265679</td>\n","      <td>0.266744</td>\n","      <td>0.267202</td>\n","      <td>0.266808</td>\n","      <td>0.266266</td>\n","      <td>0.266768</td>\n","      <td>0.267507</td>\n","      <td>0.266740</td>\n","      <td>0.265624</td>\n","      <td>0.265355</td>\n","      <td>0.264461</td>\n","      <td>0.262806</td>\n","      <td>0.262251</td>\n","      <td>0.263087</td>\n","      <td>0.264431</td>\n","      <td>0.266533</td>\n","      <td>0.269126</td>\n","      <td>...</td>\n","      <td>2.00603</td>\n","      <td>2.00192</td>\n","      <td>2.00225</td>\n","      <td>2.00244</td>\n","      <td>1.99688</td>\n","      <td>1.98540</td>\n","      <td>1.96969</td>\n","      <td>1.94942</td>\n","      <td>1.92816</td>\n","      <td>1.91071</td>\n","      <td>1.89728</td>\n","      <td>1.88298</td>\n","      <td>1.86131</td>\n","      <td>1.83355</td>\n","      <td>1.80581</td>\n","      <td>1.78410</td>\n","      <td>1.77195</td>\n","      <td>1.76479</td>\n","      <td>1.75437</td>\n","      <td>-0.630435</td>\n","      <td>-0.700000</td>\n","      <td>-0.783875</td>\n","      <td>-0.364146</td>\n","      <td>1.165479</td>\n","      <td>1.062682</td>\n","      <td>-0.716713</td>\n","      <td>-0.090016</td>\n","      <td>-0.861091</td>\n","      <td>-0.537106</td>\n","      <td>-0.722567</td>\n","      <td>-0.646673</td>\n","      <td>1.687734</td>\n","      <td>0.190708</td>\n","      <td>0.056843</td>\n","      <td>Subsoil</td>\n","      <td>-0.387442</td>\n","      <td>-0.231552</td>\n","      <td>-1.531538</td>\n","      <td>-0.264023</td>\n","      <td>1.692209</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WDId41qG</td>\n","      <td>0.317433</td>\n","      <td>0.316265</td>\n","      <td>0.314948</td>\n","      <td>0.315224</td>\n","      <td>0.316942</td>\n","      <td>0.317764</td>\n","      <td>0.316067</td>\n","      <td>0.313874</td>\n","      <td>0.313301</td>\n","      <td>0.313296</td>\n","      <td>0.313051</td>\n","      <td>0.313306</td>\n","      <td>0.314301</td>\n","      <td>0.315640</td>\n","      <td>0.316764</td>\n","      <td>0.316759</td>\n","      <td>0.315631</td>\n","      <td>0.314860</td>\n","      <td>0.314275</td>\n","      <td>0.312711</td>\n","      <td>0.311094</td>\n","      <td>0.310565</td>\n","      <td>0.311120</td>\n","      <td>0.312103</td>\n","      <td>0.312638</td>\n","      <td>0.312326</td>\n","      <td>0.311623</td>\n","      <td>0.311752</td>\n","      <td>0.312137</td>\n","      <td>0.311122</td>\n","      <td>0.309909</td>\n","      <td>0.309824</td>\n","      <td>0.309471</td>\n","      <td>0.308209</td>\n","      <td>0.307262</td>\n","      <td>0.307201</td>\n","      <td>0.307804</td>\n","      <td>0.309592</td>\n","      <td>0.312165</td>\n","      <td>...</td>\n","      <td>1.79495</td>\n","      <td>1.79606</td>\n","      <td>1.79749</td>\n","      <td>1.79798</td>\n","      <td>1.79977</td>\n","      <td>1.80183</td>\n","      <td>1.80012</td>\n","      <td>1.79366</td>\n","      <td>1.78411</td>\n","      <td>1.77356</td>\n","      <td>1.76544</td>\n","      <td>1.76124</td>\n","      <td>1.75742</td>\n","      <td>1.75113</td>\n","      <td>1.74128</td>\n","      <td>1.72894</td>\n","      <td>1.71991</td>\n","      <td>1.71562</td>\n","      <td>1.71158</td>\n","      <td>-0.753623</td>\n","      <td>-0.836364</td>\n","      <td>-0.929451</td>\n","      <td>-0.633972</td>\n","      <td>1.544098</td>\n","      <td>1.156705</td>\n","      <td>-1.282552</td>\n","      <td>-0.088336</td>\n","      <td>-0.935273</td>\n","      <td>-0.631725</td>\n","      <td>-0.832298</td>\n","      <td>-0.814516</td>\n","      <td>1.806660</td>\n","      <td>0.190708</td>\n","      <td>0.056843</td>\n","      <td>Topsoil</td>\n","      <td>-0.248601</td>\n","      <td>-0.224635</td>\n","      <td>-0.259551</td>\n","      <td>0.064152</td>\n","      <td>2.091835</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>JrrJf1mN</td>\n","      <td>0.261116</td>\n","      <td>0.259767</td>\n","      <td>0.258384</td>\n","      <td>0.259001</td>\n","      <td>0.261310</td>\n","      <td>0.262417</td>\n","      <td>0.260534</td>\n","      <td>0.258039</td>\n","      <td>0.257246</td>\n","      <td>0.257124</td>\n","      <td>0.257018</td>\n","      <td>0.257568</td>\n","      <td>0.258724</td>\n","      <td>0.260107</td>\n","      <td>0.261175</td>\n","      <td>0.261028</td>\n","      <td>0.259906</td>\n","      <td>0.259251</td>\n","      <td>0.258669</td>\n","      <td>0.257007</td>\n","      <td>0.255397</td>\n","      <td>0.255119</td>\n","      <td>0.256042</td>\n","      <td>0.257195</td>\n","      <td>0.257301</td>\n","      <td>0.256440</td>\n","      <td>0.256007</td>\n","      <td>0.256729</td>\n","      <td>0.257216</td>\n","      <td>0.256001</td>\n","      <td>0.254599</td>\n","      <td>0.254345</td>\n","      <td>0.253791</td>\n","      <td>0.252452</td>\n","      <td>0.251695</td>\n","      <td>0.252027</td>\n","      <td>0.253043</td>\n","      <td>0.254901</td>\n","      <td>0.257175</td>\n","      <td>...</td>\n","      <td>1.75317</td>\n","      <td>1.76090</td>\n","      <td>1.76944</td>\n","      <td>1.77287</td>\n","      <td>1.77080</td>\n","      <td>1.76396</td>\n","      <td>1.75453</td>\n","      <td>1.74775</td>\n","      <td>1.74264</td>\n","      <td>1.73527</td>\n","      <td>1.72770</td>\n","      <td>1.72349</td>\n","      <td>1.72149</td>\n","      <td>1.71630</td>\n","      <td>1.70737</td>\n","      <td>1.69952</td>\n","      <td>1.69356</td>\n","      <td>1.68812</td>\n","      <td>1.68178</td>\n","      <td>-0.753623</td>\n","      <td>-0.836364</td>\n","      <td>-0.929451</td>\n","      <td>-0.633972</td>\n","      <td>1.544098</td>\n","      <td>1.156705</td>\n","      <td>-1.282552</td>\n","      <td>-0.088336</td>\n","      <td>-0.935273</td>\n","      <td>-0.631725</td>\n","      <td>-0.832298</td>\n","      <td>-0.814516</td>\n","      <td>1.806660</td>\n","      <td>0.190708</td>\n","      <td>0.056843</td>\n","      <td>Subsoil</td>\n","      <td>-0.332195</td>\n","      <td>-0.318014</td>\n","      <td>-0.577548</td>\n","      <td>-0.318719</td>\n","      <td>2.118477</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ZoIitegA</td>\n","      <td>0.260038</td>\n","      <td>0.258425</td>\n","      <td>0.256544</td>\n","      <td>0.257030</td>\n","      <td>0.259602</td>\n","      <td>0.260786</td>\n","      <td>0.258717</td>\n","      <td>0.256352</td>\n","      <td>0.255902</td>\n","      <td>0.255822</td>\n","      <td>0.255720</td>\n","      <td>0.256521</td>\n","      <td>0.257968</td>\n","      <td>0.259571</td>\n","      <td>0.260714</td>\n","      <td>0.260465</td>\n","      <td>0.259352</td>\n","      <td>0.258872</td>\n","      <td>0.258484</td>\n","      <td>0.257105</td>\n","      <td>0.255502</td>\n","      <td>0.254720</td>\n","      <td>0.255194</td>\n","      <td>0.256394</td>\n","      <td>0.257036</td>\n","      <td>0.256583</td>\n","      <td>0.255867</td>\n","      <td>0.256101</td>\n","      <td>0.256414</td>\n","      <td>0.255297</td>\n","      <td>0.254055</td>\n","      <td>0.253889</td>\n","      <td>0.253455</td>\n","      <td>0.252198</td>\n","      <td>0.251296</td>\n","      <td>0.251400</td>\n","      <td>0.252441</td>\n","      <td>0.254763</td>\n","      <td>0.257593</td>\n","      <td>...</td>\n","      <td>1.74973</td>\n","      <td>1.75710</td>\n","      <td>1.76209</td>\n","      <td>1.76110</td>\n","      <td>1.75564</td>\n","      <td>1.75006</td>\n","      <td>1.74568</td>\n","      <td>1.74050</td>\n","      <td>1.73201</td>\n","      <td>1.72088</td>\n","      <td>1.70944</td>\n","      <td>1.69711</td>\n","      <td>1.68257</td>\n","      <td>1.66762</td>\n","      <td>1.65639</td>\n","      <td>1.64929</td>\n","      <td>1.64089</td>\n","      <td>1.62805</td>\n","      <td>1.61643</td>\n","      <td>-0.688406</td>\n","      <td>-0.763636</td>\n","      <td>-0.884658</td>\n","      <td>-0.583576</td>\n","      <td>1.276837</td>\n","      <td>1.191691</td>\n","      <td>-1.206971</td>\n","      <td>0.011420</td>\n","      <td>-0.906182</td>\n","      <td>-0.528757</td>\n","      <td>-0.795031</td>\n","      <td>-0.780242</td>\n","      <td>0.430513</td>\n","      <td>0.190708</td>\n","      <td>0.056843</td>\n","      <td>Topsoil</td>\n","      <td>-0.438350</td>\n","      <td>-0.010210</td>\n","      <td>-0.699135</td>\n","      <td>-0.310905</td>\n","      <td>2.164148</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 3600 columns</p>\n","</div>"],"text/plain":["       PIDN  m7497.96  m7496.04  ...        pH       SOC      Sand\n","0  XNhoFZW5  0.302553  0.301137  ... -1.129366  0.353258  1.269748\n","1  9XNspFTd  0.270192  0.268555  ... -1.531538 -0.264023  1.692209\n","2  WDId41qG  0.317433  0.316265  ... -0.259551  0.064152  2.091835\n","3  JrrJf1mN  0.261116  0.259767  ... -0.577548 -0.318719  2.118477\n","4  ZoIitegA  0.260038  0.258425  ... -0.699135 -0.310905  2.164148\n","\n","[5 rows x 3600 columns]"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"zOqAns-Lbm3F","colab_type":"code","colab":{}},"source":["def findOverallLoss(ypreds,Y):\n","  err = (ypreds.reset_index(drop=True).T - Y.reset_index(drop=True).values.squeeze()) ** 2\n","  MAPE = err.mean(axis=1)\n","  return MAPE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3lQT86QmM8e","colab_type":"text"},"source":["* n_jobs=-1 means all available cores are used in compuation"]},{"cell_type":"code","metadata":{"id":"VzKUvw0HRjdH","colab_type":"code","outputId":"d1b7af3c-d97c-4dee-d5ef-3b575826e764","executionInfo":{"status":"ok","timestamp":1589221734131,"user_tz":420,"elapsed":296153,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","import time\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(afrsoil.iloc[:,list(range(1,3594)) + [3497]], pd.DataFrame(afrsoil.loc[:,'pH']), test_size=250)\n","y_train\n","\n","start_time = time.time()\n","clf = RandomForestRegressor()\n","clf = clf.fit(X_train, y_train.values.ravel())\n","y_pred = clf.predict(X_test)\n","print('Loss:', findOverallLoss(y_test, pd.DataFrame(y_pred)))\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","start_time = time.time()\n","clf = RandomForestRegressor(n_jobs=-1)\n","clf = clf.fit(X_train, y_train.values.ravel())\n","y_pred = clf.predict(X_test)\n","print('Parallel loss:', findOverallLoss(y_test, pd.DataFrame(y_pred)))\n","\n","print(\"--- %s seconds in parallel ---\" % (time.time() - start_time))\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Loss: pH    0.225813\n","dtype: float64\n","--- 166.37320971488953 seconds ---\n","Parallel loss: pH    0.229848\n","dtype: float64\n","--- 128.6044135093689 seconds in parallel ---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-iezgyzFl_CV","colab_type":"text"},"source":["* That wasnt much faster, but lets try a max on the number of features considered for each tree..."]},{"cell_type":"code","metadata":{"id":"qXKk-mtxksaD","colab_type":"code","outputId":"ce4ac97a-00e5-44d9-a8ef-0b173031504a","executionInfo":{"status":"ok","timestamp":1589072632455,"user_tz":420,"elapsed":349168,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["start_time = time.time()\n","clf = RandomForestRegressor(n_jobs=-1, max_features=1000)\n","clf = clf.fit(X_train, y_train.values.ravel())\n","y_pred = clf.predict(X_test)\n","print('Parallel accuracy (with max val on features):', findOverallLoss(y_test, pd.DataFrame(y_pred)))\n","\n","print(\"--- %s seconds in parallel (with max val on features) ---\" % (time.time() - start_time))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Parallel accuracy (with max val on features): pH    0.182046\n","dtype: float64\n","--- 38.39659237861633 seconds in parallel (with max val on features) ---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_tGWJLE1ksKW","colab_type":"text"},"source":["That is way faster (less than 25% of the time) with slightly better accuracy!\n","\n","* this is because reducing features decreases variance and saves time spent comparing features"]},{"cell_type":"markdown","metadata":{"id":"evFp0PeXqBUF","colab_type":"text"},"source":["* lets compare this to guessing the mean pH\n","\n","* way better!"]},{"cell_type":"code","metadata":{"id":"BAF6DN-dZ1_g","colab_type":"code","outputId":"247008d9-61c2-4010-e2db-8fa8fdfc49ab","executionInfo":{"status":"ok","timestamp":1589224616014,"user_tz":420,"elapsed":589,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.metrics import mean_squared_error\n","#y_pred = pd.DataFrame(y_pred.loc[:,'pH'])\n","m=abs(afrsoil.loc[:,'pH'].mean())\n","abs(y_test.subtract(m)).mean()\n"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pH    0.658468\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"vETPyobLoNZW","colab_type":"text"},"source":["## 5.3. Boosting: Repeatedly Tweaking a Tree"]},{"cell_type":"markdown","metadata":{"id":"rXt6epKhr1g-","colab_type":"text"},"source":["* eg. imagine if we had 12.5 as threshold value but more would be predicted correct if it was slightly modified to 11.9 - this is the idea of boosting\n","\n","  * tweak tree to form new tree ***s*** times"]},{"cell_type":"markdown","metadata":{"id":"hdsWnC2mswNw","colab_type":"text"},"source":["### 5.3.1. Adaboost Implementation"]},{"cell_type":"markdown","metadata":{"id":"kq0p-j2sofVr","colab_type":"text"},"source":["* in the ***adaptive boosting*** method, we *tweak* the trees (by a ***weight*** equal to the amount of the prediction error in the previous iteration) so that the next tree will adapt to the threshold on each split \n","\n","  * the prediction accuracy with each new tree increases because the higher weighted values have more weight in the calculation of whether or not to make a split thus reducing overall error\n","\n","  * we form new trees based on the old ones until we have ***s*** trees (where ***s*** is a hyperparameter)\n","\n","* the sub-trees within ***s*** are called ***weak learners*** because the final prediction made by the boosting classifier is their weighted sum\n","\n","\n","\n","**sklearn**\n","\n","  * the number of weak learners (***s***) is set in `n_estimators` \n","  \n","  * we may also often want change to change the `max_depth`, `min_samples_split`, and `base_estimator` paremeters to improve accuracy\n","\n","    * the default base estimators are *'stumps'* (one-level DT's)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DzFK8HCptD-F","colab_type":"text"},"source":["**algoithm:**\n","\n","* we start with a weight **1/n** for each sample of our **n** data points \n","\n","* in each iteration (up to ***s*** iterations) weights are updated for each tree (according to the error in the previous one)\n","\n","  * the decision trees will grow in favor splitting sets of samples with high weights\n"]},{"cell_type":"code","metadata":{"id":"hXHv0ptwsXZd","colab_type":"code","outputId":"15742fd0-bf82-4c31-99f6-a5dbd954cdef","executionInfo":{"status":"ok","timestamp":1588898245711,"user_tz":420,"elapsed":1933,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import load_iris\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","X, y = load_iris(return_X_y=True)\n","clf = AdaBoostClassifier(n_estimators=100)\n","scores = cross_val_score(clf, X, y, cv=5)\n","scores.mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9466666666666665"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"fuVCMxKOt7I7","colab_type":"text"},"source":["### 5.3.2. Gradient Boosting (AKA Gradient Boosted Decision Trees (GBDT))"]},{"cell_type":"markdown","metadata":{"id":"q7PbtYfeuFhY","colab_type":"text"},"source":["* we fit the data to the ***residuals*** (difference between predicted an actual value)\n","\n","**algorithm**\n","\n","  * 1. start with 1 tree (CurrentTree)\n","\n","  * 2. calculate residual for each data point\n","\n","  * 3. fit new tree T to residuals (eg. as the data) set new tree to CurrentTree=T \n","\n","    * so we are pretty much working on the \"leftover\" that we couldnt predict  in the last iteration\n","\n","* Then we form our prediction as the sum of all the trees\n","\n","  * 4. go to step 2\n","\n","* it is called gradient since we are minimizing the partial derivatives of loss function \n","\n"]},{"cell_type":"markdown","metadata":{"id":"_50Zs9ES-U4R","colab_type":"text"},"source":["![alt text](https://miro.medium.com/max/1400/1*fHenn7NVqcWvw25D3-zRiQ.png)\n","\n","![alt text](https://miro.medium.com/max/1400/1*LLbC4TstqzXQ3hzA8wCmeg.png)"]},{"cell_type":"markdown","metadata":{"id":"huNiC4v0Jcea","colab_type":"text"},"source":["### 5.3.3. Sklearn GradientBoostingRegressor and GradientBoostingClassifier"]},{"cell_type":"markdown","metadata":{"id":"FOqzYsUyJrF_","colab_type":"text"},"source":["* the aim of this kaggle ds is to predict the quality of a call rating "]},{"cell_type":"code","metadata":{"id":"eBZwPk2g-bTC","colab_type":"code","outputId":"bca3a59e-1d99-42f4-f599-70aac787e03f","executionInfo":{"status":"ok","timestamp":1589224848952,"user_tz":420,"elapsed":11764,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["import pandas as pd\n","import numpy as np\n","### INSERT YOUR PATH to data HERE: ###\n","my_path = '/content/drive/My Drive/ecs171_yancey/Lecture_Notes/Chapter_5/dataset.xlsx'\n","\n","ds = pd.read_excel(my_path)\n","\n","ds.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date Of Test</th>\n","      <th>Signal (dBm)</th>\n","      <th>Speed (m/s)</th>\n","      <th>Distance from site (m)</th>\n","      <th>Call Test Duration (s)</th>\n","      <th>Call Test Result</th>\n","      <th>Call Test Technology</th>\n","      <th>Call Test Setup Time (s)</th>\n","      <th>MOS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2017-07-01 00:00:27</td>\n","      <td>-61.0</td>\n","      <td>68.800003</td>\n","      <td>1048.60</td>\n","      <td>90.0</td>\n","      <td>SUCCESS</td>\n","      <td>UMTS</td>\n","      <td>0.56</td>\n","      <td>2.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2017-07-01 00:02:57</td>\n","      <td>-61.0</td>\n","      <td>68.769997</td>\n","      <td>1855.54</td>\n","      <td>90.0</td>\n","      <td>SUCCESS</td>\n","      <td>UMTS</td>\n","      <td>0.45</td>\n","      <td>3.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2017-07-01 00:05:29</td>\n","      <td>-71.0</td>\n","      <td>69.169998</td>\n","      <td>1685.62</td>\n","      <td>90.0</td>\n","      <td>SUCCESS</td>\n","      <td>UMTS</td>\n","      <td>0.51</td>\n","      <td>2.1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2017-07-01 00:08:02</td>\n","      <td>-65.0</td>\n","      <td>69.279999</td>\n","      <td>1770.92</td>\n","      <td>90.0</td>\n","      <td>SUCCESS</td>\n","      <td>UMTS</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2017-07-01 00:10:30</td>\n","      <td>-103.0</td>\n","      <td>0.820000</td>\n","      <td>256.07</td>\n","      <td>60.0</td>\n","      <td>SUCCESS</td>\n","      <td>UMTS</td>\n","      <td>3.35</td>\n","      <td>3.6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Date Of Test  Signal (dBm)  ...  Call Test Setup Time (s)  MOS\n","0 2017-07-01 00:00:27         -61.0  ...                      0.56  2.1\n","1 2017-07-01 00:02:57         -61.0  ...                      0.45  3.2\n","2 2017-07-01 00:05:29         -71.0  ...                      0.51  2.1\n","3 2017-07-01 00:08:02         -65.0  ...                      0.00  1.0\n","4 2017-07-01 00:10:30        -103.0  ...                      3.35  3.6\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"mI0VlG0Gvdqk","colab_type":"text"},"source":["* alot of dummies not read in as such need to be converted for the alg"]},{"cell_type":"code","metadata":{"id":"5yUvIe4jER3D","colab_type":"code","colab":{}},"source":["ds=pd.get_dummies(ds,drop_first=True)\n","\n","ds= ds.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhJzsoq9vnge","colab_type":"text"},"source":["* GradientBoostingClassifier for classification\n","\n","* also need to use `values.ravel()` on the `y_train` for this"]},{"cell_type":"code","metadata":{"id":"pd7hihWw_iNh","colab_type":"code","outputId":"4ac07ef3-5bb9-4f27-cd22-23eb8aa0880b","executionInfo":{"status":"ok","timestamp":1589225111570,"user_tz":420,"elapsed":11222,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","X_train, X_test, y_train, y_test = train_test_split(ds.iloc[:,1:11],pd.DataFrame(ds.loc[:,'MOS']) )\n","\n","clf = GradientBoostingRegressor(n_estimators=500).fit(X_train, y_train.values.ravel())\n","y_pred= clf.predict(X_test)\n","mean_squared_error(y_test, y_pred)\n"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.013851103693229e-07"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"-6wLbwzgwDxr","colab_type":"text"},"source":["* lets compare to adaboost"]},{"cell_type":"code","metadata":{"id":"NzTZg9yRwCzn","colab_type":"code","outputId":"f39ccc2f-c429-4e21-af18-1ba690c3ba30","executionInfo":{"status":"ok","timestamp":1589225096449,"user_tz":420,"elapsed":2952,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.ensemble import AdaBoostRegressor\n","\n","clf = AdaBoostRegressor(n_estimators=500).fit(X_train, y_train.values.ravel())\n","y_pred= clf.predict(X_test)\n","mean_squared_error(y_test, y_pred)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.002469965686340037"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"LNxEgRq4wX33","colab_type":"text"},"source":["\n","* not as good\n","\n","* may be more significant with more data"]},{"cell_type":"markdown","metadata":{"id":"f3_rhAUt_dpY","colab_type":"text"},"source":["### Histogram-based Boosting"]},{"cell_type":"markdown","metadata":{"id":"SVFaz0YT_ob1","colab_type":"text"},"source":["* experimental/new from sklearn\n","\n","* the samples are split into 256 bins\n","\n","* this speeds up training (from GBDT) by reducing the number of splitting points that need to be considered"]},{"cell_type":"code","metadata":{"id":"132F5q8U_dDO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4b73e061-f35d-4470-ad49-0c95eae19af8","executionInfo":{"status":"ok","timestamp":1589225084943,"user_tz":420,"elapsed":1635,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}}},"source":["from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingRegressor\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(ds.iloc[:,1:11],pd.DataFrame(ds.loc[:,'MOS']) )\n","\n","clf = HistGradientBoostingRegressor(max_iter=100).fit(X_train, y_train.values.ravel())\n","y_pred= clf.predict(X_test)\n","mean_squared_error(y_test, y_pred)\n"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.101523061493721e-09"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"RH4wB5gzMz3F","colab_type":"text"},"source":["* best !"]},{"cell_type":"markdown","metadata":{"id":"ijkRp1Fkdwuc","colab_type":"text"},"source":["5.3.3.1. Tuning Parameters in GradientBoostingRegressor"]},{"cell_type":"markdown","metadata":{"id":"3Y63frgEd5gx","colab_type":"text"},"source":["* `loss` argument allows us to specify loss type (least squares is default (`ls`))\n"," \n","\n","**determining the optimal # of trees:**\n","\n","* `train_score_` attribute holds the train error at each iteration; while `staged_predict`\n","returns a generator that yields the predictions on the test set at each stage\n","\n","* `feature_importances_` hold feature importances \n","\n","*  `warm_start=True` which allows you to add more estimators to model that has already been fit\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pKOAqymGpwpW","colab_type":"text"},"source":["* let’s see how the approximation progresses as we add more trees \n","\n","* use he `staged_(predict|predict_proba)` methods to evaluate the prediction of a model as a function of the number of trees via \n","\n","   * it will return a generator that iterates over the predictions"]},{"cell_type":"code","metadata":{"id":"iddR02_KzvhP","colab_type":"code","outputId":"91f1fee0-6223-4403-bc27-e14a1a4962ba","executionInfo":{"status":"ok","timestamp":1589096016605,"user_tz":420,"elapsed":977,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import mean_squared_error\n","errors = [mean_squared_error(y_test, y_pred) for y_pred in clf.staged_predict(X_test)]\n","best_n_estimators = np.argmin(errors)\n","best_n_estimators"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["120"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"AKBtfYvC3OP4","colab_type":"code","outputId":"1b2a86dc-87a1-4365-e5e9-05bcf0f9240f","executionInfo":{"status":"ok","timestamp":1589096027307,"user_tz":420,"elapsed":10679,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["clf = GradientBoostingRegressor(\n","    n_estimators=best_n_estimators\n",")\n","clf.fit(X_train, y_train.values.ravel())\n","y_pred = clf.predict(X_test)\n","mean_squared_error(y_test, y_pred)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.745658509564754e-08"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"NgL88yeKBJk8","colab_type":"text"},"source":["* way improved!"]},{"cell_type":"markdown","metadata":{"id":"Tc81Xjlq3ks2","colab_type":"text"},"source":["![alt text](https://miro.medium.com/max/1028/1*OR_dzailAOI5FfBjD__V_g.png)\n","source: https://scikit-learn.org/stable/modules/model_evaluation.html\n","\n","* no need to write our own loss functions anymore: `sklearn` provides many ways to evaluate accuracy including a table (showing each type of scoring)"]},{"cell_type":"markdown","metadata":{"id":"nKG_7Ov21bzz","colab_type":"text"},"source":["### Feature importance evaluation"]},{"cell_type":"code","metadata":{"id":"I4FoHGuXRWcb","colab_type":"code","outputId":"14156c31-c8ff-4620-e7bb-b92323193996","executionInfo":{"status":"ok","timestamp":1588991500412,"user_tz":420,"elapsed":384,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["clf.feature_importances_"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.79234258e-11, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","       0.00000000e+00, 0.00000000e+00])"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"qpDvSqwGdcwt","colab_type":"code","outputId":"98415fd2-c045-4b31-fc3a-bc571b285749","executionInfo":{"status":"ok","timestamp":1588991528553,"user_tz":420,"elapsed":550,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","importances = clf.feature_importances_\n","\n","plt.figure()\n","plt.title(\"Feature importances\")\n","plt.bar(range(X_test.shape[1]), importances[indices],\n","       color=\"r\", yerr=std[indices], align=\"center\")\n","plt.xticks(range(X_test.shape[1]), indices)\n","plt.xlim([-1, X_test.shape[1]])\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"6qjDG98Ihm6W","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"AeB4I8kVQn-j","colab_type":"text"},"source":["### 5.3.4. Bias vs. Variance in Boosting"]},{"cell_type":"markdown","metadata":{"id":"J58h_NVKQsKv","colab_type":"text"},"source":["* ***boosting*** is averaging many trees (making it more stable)\n","\n","* making small adjustments to the trees is used to reduce bias by developing a more detailed analysis\n","\n","\n","* but sometimes ***s*** is too large and we overfit"]},{"cell_type":"markdown","metadata":{"id":"agMBT_jvRkQz","colab_type":"text"},"source":["5.3.5. Computational speed"]},{"cell_type":"markdown","metadata":{"id":"qgxLBLx1RocC","colab_type":"text"},"source":["* can take many cpu cycles\n","\n","* often good to try multicore in boosting"]},{"cell_type":"markdown","metadata":{"id":"hsylsFw_Rvkh","colab_type":"text"},"source":["### 5.3.6. Hyperparamters"]},{"cell_type":"markdown","metadata":{"id":"jEF3fOENSIiw","colab_type":"text"},"source":["* increasing `n_estimators` or decreasing `min_samples_leaf` in boosting will generally reduce bias but increase variance\n","\n"]},{"cell_type":"markdown","metadata":{"id":"32r7dkuPSfRv","colab_type":"text"},"source":["### 5.3.7. The Learning Rate"]},{"cell_type":"markdown","metadata":{"id":"Wwx7HqfySkaZ","colab_type":"text"},"source":["* controlls overfitting via *shrinkage*\n","\n","* we will also use this in SVM and NN soon :)"]},{"cell_type":"markdown","metadata":{"id":"TYKIlTB3S092","colab_type":"text"},"source":["#### 5.3.7.1. General Concepts"]},{"cell_type":"markdown","metadata":{"id":"K6doHrXFS5jF","colab_type":"text"},"source":["![alt text](https://employees.csbsju.edu/cschaller/Principles%20Chem/conformation/globalmin.gif)\n","\n","\n","* the algorothim starts with a random guess and iteritively moves toward the minimum of the function\n","\n","* moves in the direction of the negative gradient until it converges at ~0 grad (change from one step to the next or 0 slope\n","\n","\n","* **lr** tells us *how much* to increment in the direction of the downward slope in each iteration\n","\n","* unfortunately it will lead us to local minimum instead if we dont start at near the global by accident\n","\n","* also smaller lr are preferrable so we dont *miss* the minimum (eg. move all the way accross to the other direction of gradient\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bDQ5BZG8VsXg","colab_type":"text"},"source":["##### The learning rate in gradient boosting"]},{"cell_type":"markdown","metadata":{"id":"fpAf8SaEWBTv","colab_type":"text"},"source":["* in this case the LR is multiplied by each of the residuals that form the new tree values\n","\n","* a smaller learning rate may require more trees but reduce overfitting"]},{"cell_type":"markdown","metadata":{"id":"OFhEF8oCWgEo","colab_type":"text"},"source":["## 5.4. Pitfall: NO free lunch"]},{"cell_type":"markdown","metadata":{"id":"zA9lPqlvWk7q","colab_type":"text"},"source":["* Adaboost is s good 'off-the-shelf' algorithm but there is still always the possibility of over-fitting (with any ml algorithm)\n","\n","* eg. even if we choose a very high value of ***s***, we could over-fit the individual trees"]},{"cell_type":"markdown","metadata":{"id":"UPrewhsgOhCq","colab_type":"text"},"source":["\n","* good tuning tips for the homework: https://medium.com/@taplapinger/tuning-a-random-forest-classifier-1b252d1dde92"]}]}