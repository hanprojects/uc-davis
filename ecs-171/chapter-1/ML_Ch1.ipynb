{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Ch1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOGg9UthkYN+RxKgYsOPQHI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P-qGyxklFqKW","colab_type":"text"},"source":["#  Quick Intro to Google Colab"]},{"cell_type":"markdown","metadata":{"id":"OFf_a1IB1xs0","colab_type":"text"},"source":["* In ML you will often have many large data sets\n","\n","- at a certain point you won't be able to fit it all on 1 CPU, for example\n","\n","- Mounting your drive can be very helpful to store and manage data\n","\n","- plus its safe and accessible anywhere"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"byjJUqbOq29C","outputId":"3f0614ce-f7a0-4241-9c35-f63994a296b3","executionInfo":{"status":"ok","timestamp":1587789740177,"user_tz":420,"elapsed":495,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# import drive like this or click the pictue of the file folder on the left side panel:\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HXZ28UzW2GcX","colab_type":"text"},"source":["In Colab NB you can run **command line** \n","statements with **!**.\n","\n","Here are some examples showing how it can be useful:\n"]},{"cell_type":"markdown","metadata":{"id":"8CgGVBmALeID","colab_type":"text"},"source":["* navigate through or create new directories in your drive:\n","\n"]},{"cell_type":"code","metadata":{"id":"qRA52MexGg9N","colab_type":"code","outputId":"450622a2-0a7e-4ca4-b3d7-41b992e6816c","executionInfo":{"status":"ok","timestamp":1587789742155,"user_tz":420,"elapsed":2457,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# import operating system and change to directory\n","import os\n","os.chdir('/content/drive/My Drive/ecs171_yancey/Lecture_Notes/Chapter_1/')\n","# note that you wont be able to write files to my shared folder \n","# (but you can make/copy your own folder on your drive if you want)\n","! mkdir 'test_files'\n","! ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘test_files’: File exists\n","day1.csv  ML_Ch1.ipynb\ttest_files  test.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w2Yiwow0PD0b","colab_type":"text"},"source":["You usually wont need to, but if any libraries are not already installed (cannot be directly imported); just use:\n","\n","`!pip install`"]},{"cell_type":"markdown","metadata":{"id":"u9zDgRplLZoG","colab_type":"text"},"source":["* run other Python files (esp. old file not in a notebook and/or without GPU):"]},{"cell_type":"code","metadata":{"id":"L-sftJLzK3f-","colab_type":"code","outputId":"9ad7c141-975c-4abd-8820-3eca40c37b46","executionInfo":{"status":"ok","timestamp":1587789743071,"user_tz":420,"elapsed":3361,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# run python file already in drive directory\n","! python test.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":[".py python test file running!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LK6o4XtaK2zh","colab_type":"text"},"source":["* create text files in drive, to read/write"]},{"cell_type":"code","metadata":{"id":"mZtCssqkK12T","colab_type":"code","outputId":"94ffc97a-5bb3-4f4c-b791-506144494b36","executionInfo":{"status":"ok","timestamp":1587789743261,"user_tz":420,"elapsed":3546,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# create a file in drive\n","os.chdir('./test_files/')\n","with open('test2.txt', 'w') as f:\n","  f.write('Test file #2 exists in drive!')\n","\n","# read it\n","f = open(\"test2.txt\", \"r\")\n","print(f.read())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test file #2 exists in drive!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RXpWTeqLLl9P","colab_type":"text"},"source":["* download data from internet into drive:"]},{"cell_type":"code","metadata":{"id":"ErxOQb7FH1mS","colab_type":"code","colab":{}},"source":["# get from internet\n","!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n","# unzip\n","!unzip -qq flower_data.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sq8jcr4kR569","colab_type":"code","colab":{}},"source":["# clean up \n","os.rmdir(\"test_files\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SkA1HGrH2cqd","colab_type":"text"},"source":["Also for **free GPU** :\n","\n","From task bar: Runtime -> Change runtime type\n","\n","Hardware accelerator: None -> GPU\n","\n","* We wont need this *yet*, but in ML you will often need a GPU to train your model because you will have large data sets and will want to train as fast as possible (or it could take days on a CPU)"]},{"cell_type":"code","metadata":{"id":"lW854k9GHDL3","colab_type":"code","colab":{}},"source":["# use Tensorflow to detect GPU\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBpyJcYWOHy7","colab_type":"text"},"source":["- can also switch between **Python 2 and 3** (in Runtime Type)\n","- or download **.py or .pynb** (under File menu)\n","- and discover lots more cool features on your own! (if new to Colab)"]},{"cell_type":"markdown","metadata":{"id":"W8IB7qDgJCO7","colab_type":"text"},"source":["# Quick Intro to Data Analysis Libraries (for ML in Python)"]},{"cell_type":"markdown","metadata":{"id":"PJRKTv17muGK","colab_type":"text"},"source":["**data processing** is essential to machine learning so it is important to be aware of the libraries we can use to help us do it efficiently"]},{"cell_type":"markdown","metadata":{"id":"ZZRhlBAVjdJA","colab_type":"text"},"source":["* optimized/faster matrix and vector operations\n","\n","* methods to assign, access, and manipulate data slices quickly and efficiently\n","\n","* part of the \"Python *machine learning ecosystem*\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g0_n8sB4kegl","colab_type":"text"},"source":["### NumPy"]},{"cell_type":"markdown","metadata":{"id":"iLJpRj5V3BWG","colab_type":"text"},"source":["\n","1. **NumPy** (numeric python)\n","\n","  * easy methods to apply to scientific data and do mathematical operations\n","  * best for multi-dim arrays (n axes is the 'rank')\n","  * homogeneous type elements (usually integers)\n","  * best for element-wise calculations, matrix operations\n","  * 50X faster than Python lists (because of 'locality of reference' and parts written in C/C++)\n"]},{"cell_type":"code","metadata":{"id":"lBVr6HNrq24P","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# create large arrays quick without loops, eg.\n","arr = np.random.rand(1000,1000)\n","\n","# similar syntax \n","arr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]], dtype='float')\n","\n","# number of dims (just one np method example)\n","print(arr.shape)\n","\n","# faster access:\n","arr[0, 1, 2]\n","\n","# see docs for other methods, if this is new"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o8y4kVhZlyg4","colab_type":"text"},"source":["### Pandas"]},{"cell_type":"markdown","metadata":{"id":"gezLIaQdowhK","colab_type":"text"},"source":["2. **Pandas**: \n","\n","  * best for 2-D table-like or spreadsheet data-frames\n","  * easy to export/import CSV, Excel, JSON, HTML and SQL database\n","  * easy to add, remove, sort, ect. columns or rows"]},{"cell_type":"code","metadata":{"id":"37vAkfRkAdvH","colab_type":"code","outputId":"66de254f-ea45-45f8-fc0c-dc166c6f5e52","executionInfo":{"status":"ok","timestamp":1587790141462,"user_tz":420,"elapsed":513,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":142}},"source":["import pandas as pd\n","\n","# pandas series is a 1-D vector\n","# can create a pandas DF from a dict of series (where keys become column names)\n","activity_dict = { \"weight\": pd.Series([130, 200, 112],index=[\"Jeff\", \"John\", \"Morgan\"]),\n","               \"age\": pd.Series([84, 35, 28], index=[\"Jeff\", \"John\", \"Morgan\"]), \n","              \"sport\": pd.Series([\"Biking\", \"Dancing\"], index=[\"John\", \"Jeff\"]),}\n","\n","activity = pd.DataFrame(activity_dict)\n","activity"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>weight</th>\n","      <th>age</th>\n","      <th>sport</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Jeff</th>\n","      <td>130</td>\n","      <td>84</td>\n","      <td>Dancing</td>\n","    </tr>\n","    <tr>\n","      <th>John</th>\n","      <td>200</td>\n","      <td>35</td>\n","      <td>Biking</td>\n","    </tr>\n","    <tr>\n","      <th>Morgan</th>\n","      <td>112</td>\n","      <td>28</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        weight  age    sport\n","Jeff       130   84  Dancing\n","John       200   35   Biking\n","Morgan     112   28      NaN"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"VC054dhXgLin","colab_type":"text"},"source":["* often times we have data frames with **NA** values and we dont even know\n","* then our ML algorithm will return an error (so we need to fix it somehow first)\n","* this is how we can find out how many of them are in each feature column"]},{"cell_type":"code","metadata":{"id":"C-_L0FaObIMl","colab_type":"code","outputId":"09195588-5ec2-4a2a-d19a-ad5c666573d7","executionInfo":{"status":"ok","timestamp":1587790143889,"user_tz":420,"elapsed":420,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# see \n","activity.isnull().sum()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["weight    0\n","age       0\n","sport     1\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"B9BtXJ2egia_","colab_type":"text"},"source":["* sport column has 1 NA\n","* if we still want to include this row in our test or train data, there are a number of ways we might want to handle this\n","* one may be to fill it in with the most common sport (because it might be most likely anyways)\n","* so lets count how many there are of each sport in our df"]},{"cell_type":"code","metadata":{"id":"eAQ8h2HhQs1A","colab_type":"code","colab":{}},"source":["activity[\"sport\"].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PcFDLR-tf65a","colab_type":"text"},"source":["* replace rows in sport col with NA with \"Dancing\""]},{"cell_type":"code","metadata":{"id":"_MCFB-zHQxQb","colab_type":"code","colab":{}},"source":["activity[\"sport\"] = activity[\"sport\"].fillna(\"Dancing\")\n","activity"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SXuojKkJldEz","colab_type":"text"},"source":["* if our missing value was an interger such as in the weight col we might fill with the mean\n","* there is also a method for that"]},{"cell_type":"code","metadata":{"id":"oCHLYMM4f_b8","colab_type":"code","colab":{}},"source":["activity[\"weight\"].mean()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ebvqrgGDlnK9","colab_type":"text"},"source":["* also often will need to locate rows in columns having certain values or meeting certain conditions\n","* eg. turn to it into an **indicator variable** column for Dancing "]},{"cell_type":"code","metadata":{"id":"ouRMVz3zbgGS","colab_type":"code","colab":{}},"source":["activity.loc[activity[\"sport\"] != \"Dancing\",\"sport\"] = 0\n","activity.loc[activity[\"sport\"] == \"Dancing\",\"sport\"] = 1\n","activity"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKabBKG1mGfU","colab_type":"text"},"source":["# Chapter 1: Regression"]},{"cell_type":"markdown","metadata":{"id":"b9nzWojBnVX2","colab_type":"text"},"source":["## 1.2. The Bike Sharing Dataset"]},{"cell_type":"markdown","metadata":{"id":"C2_I6nBsnTaw","colab_type":"text"},"source":["\n","* Matloff's slightly modified version of data available on the **UCI Machine Learning** website\n","  * numeric weather variables in original scale\n","\n","* `.head()` function is used to view the top of the data so you can see the column names, first 5 rows"]},{"cell_type":"code","metadata":{"id":"sACwJavLTR3i","colab_type":"code","outputId":"e15f2992-54f2-4c3a-cd29-29a330e77d14","executionInfo":{"status":"ok","timestamp":1587790148710,"user_tz":420,"elapsed":762,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["# You may need to modify your path based on where you \n","# copy stuff from ecs171_yancey in your drive\n","# but I shared this day1.csv dataset in 'ecs171_yancey/Lecture_Notes/week_1/':\n","my_path = '/content/drive/My Drive/ecs171_yancey/Lecture_Notes/Chapter_1/day1.csv'\n","\n","# load .csv using Pandas library\n","day1 = pd.read_csv(my_path)\n","\n","# look at the first few rows and show features \n","day1.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>instant</th>\n","      <th>dteday</th>\n","      <th>season</th>\n","      <th>yr</th>\n","      <th>mnth</th>\n","      <th>holiday</th>\n","      <th>weekday</th>\n","      <th>workingday</th>\n","      <th>weathersit</th>\n","      <th>temp</th>\n","      <th>atemp</th>\n","      <th>hum</th>\n","      <th>windspeed</th>\n","      <th>casual</th>\n","      <th>registered</th>\n","      <th>tot</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2011-01-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>8.175849</td>\n","      <td>7.999250</td>\n","      <td>0.805833</td>\n","      <td>10.749882</td>\n","      <td>331</td>\n","      <td>654</td>\n","      <td>985</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2011-01-02</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>9.083466</td>\n","      <td>7.346774</td>\n","      <td>0.696087</td>\n","      <td>16.652113</td>\n","      <td>131</td>\n","      <td>670</td>\n","      <td>801</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2011-01-03</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.229108</td>\n","      <td>-3.499270</td>\n","      <td>0.437273</td>\n","      <td>16.636703</td>\n","      <td>120</td>\n","      <td>1229</td>\n","      <td>1349</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2011-01-04</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.400000</td>\n","      <td>-1.999948</td>\n","      <td>0.590435</td>\n","      <td>10.739832</td>\n","      <td>108</td>\n","      <td>1454</td>\n","      <td>1562</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2011-01-05</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2.666979</td>\n","      <td>-0.868180</td>\n","      <td>0.436957</td>\n","      <td>12.522300</td>\n","      <td>82</td>\n","      <td>1518</td>\n","      <td>1600</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  instant      dteday  season  ...  windspeed  casual  registered   tot\n","0           1        1  2011-01-01       1  ...  10.749882     331         654   985\n","1           2        2  2011-01-02       1  ...  16.652113     131         670   801\n","2           3        3  2011-01-03       1  ...  16.636703     120        1229  1349\n","3           4        4  2011-01-04       1  ...  10.739832     108        1454  1562\n","4           5        5  2011-01-05       1  ...  12.522300      82        1518  1600\n","\n","[5 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"wLz6tSmbDZ4g","colab_type":"code","colab":{}},"source":["day1.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TXbDvPefrQCy","colab_type":"text"},"source":["### 1.2.2. Dummy (Indicator) Variables"]},{"cell_type":"markdown","metadata":{"id":"JEY5IAM2tY3N","colab_type":"text"},"source":["* Take on 1 or 0 only \n","  * Eg **Urban, Rural, Suburban** may be represented as **1, 2, 3** (even though one is not more than the other) because they are *categorical variables*\n","\n","  * So we need to convert these to dummies so that the ML algorithm knows they are \n","    * eg. so it knows that 1 is not \"less good\" than 3\n","* can use `pd.get_dummies()`"]},{"cell_type":"markdown","metadata":{"id":"tQ6LkX_iunxF","colab_type":"text"},"source":["## 1.3. Prediction, 1.4 Classification, 1.5. Stats vs. ML"]},{"cell_type":"markdown","metadata":{"id":"ycXMvVTVuvAK","colab_type":"text"},"source":["* Predictor variables are called ***features***\n","\n","* Helpful way to predict the future \n","\n","  * eg. Market demand, disease progression\n","\n","* but not always about the future\n","\n","  * eg. Sometimes we predict things that happened in the past or present where we don’t know what actually happened\n","\n","* ***Classification*** is a type of prediction in which we are predicting a dummy variable\n","\n","  * or a multiclass problem we could predict multiple dummy variables\n","\n","* Some prediction methods originally developed in ML (**SVM, NN, ect.**), while others originally from stats (**KNN, RF, Decision Trees, Log Regression**)"]},{"cell_type":"markdown","metadata":{"id":"Rk6SJJkWvgxN","colab_type":"text"},"source":["## 1.6. Predictions with a Single Feature"]},{"cell_type":"markdown","metadata":{"id":"bou99LaGvqpA","colab_type":"text"},"source":["* If we were to predict ridership using **KNN** with a single feature (eg. temperature is 28 degrees) we would…\n","  * 1) gather all the data points with temperatures closest to 28\n","  * 2) average the ridership on that day\n","\n","* this would be our prediction for a 28 degree day where we don’t know how many riders there was or will be\n","\n","* lets try with **k=5**..."]},{"cell_type":"code","metadata":{"id":"rISCG5xucb-A","colab_type":"code","colab":{}},"source":["# extract only the temperature feature column using .loc \n","tmps = day1.loc[:,['temp']]\n","print(tmps.head())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9dyiOc_4rlL","colab_type":"text"},"source":["* lets find the distances of each temperature to 28 degrees"]},{"cell_type":"code","metadata":{"id":"rAdYYxklEKfq","colab_type":"code","colab":{}},"source":["# distances of the temps to 28\n","dists = abs(tmps - 28)\n","# show some examples\n","print(dists.head())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcxUzWsp1nLG","colab_type":"text"},"source":["* indices of closest 5 temperatures to 28 can be found by finding the 5 smallest in `dists`\n","\n","* smallest values on top when sorted least to greatest"]},{"cell_type":"code","metadata":{"id":"H1W2T4gZESjU","colab_type":"code","colab":{}},"source":["do5 = dists.sort_values(by='temp')[0:5]\n","do5\n","# just another way to sort shown below in case interested in np\n","# do5 = np.argsort(dists.temp, axis=0)[0:5]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Bqm_Krd498a","colab_type":"text"},"source":["* how close are they?"]},{"cell_type":"code","metadata":{"id":"3RXIcyRGEWhs","colab_type":"code","colab":{}},"source":["# show how close they are to 28 degrees\n","print(do5.temp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFi28-uX5Apv","colab_type":"text"},"source":["* lets compare the indices to those found in **R** (like in the textbook, for fun or to check)"]},{"cell_type":"code","metadata":{"id":"3jDWW7LUEZYI","colab_type":"code","colab":{}},"source":["# show indices:\n","print(list(do5.index))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVpoAmvT5q01","colab_type":"text"},"source":[" * although  **R** chose 1 index different it held the same temp val\n"," * note that this will row will have a different total rider val though"]},{"cell_type":"code","metadata":{"id":"4uqrwaW3BM4x","colab_type":"code","colab":{}},"source":["print(tmps.temp[151]) # R\n","print(tmps.temp[608]) # Python\n","# but they are the same: "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"339ZNN3X6Jj7","colab_type":"text"},"source":["* rider numbers for 5 closest temps can then be used to approximate the new temp's day # of riders"]},{"cell_type":"code","metadata":{"id":"fqp3RDd9Ebqu","colab_type":"code","colab":{}},"source":["# show the actual total numbers of riders for closest neighbors\n","print(list(day1.tot[list(do5.index)]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"opw0fVyR6cWk","colab_type":"text"},"source":["* (similar to using **KNN**) we can predict the new output using the average of those"]},{"cell_type":"code","metadata":{"id":"ZfSNgv22EeB8","colab_type":"code","colab":{}},"source":["# importing mean() \n","from statistics import mean \n","# show the predicted value using the average of the neighbors' totals\n","print(mean(list(day1.tot[list(do5.index)])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_dzf0bac7Vew","colab_type":"text"},"source":["## 1.7. Bias vs. Variance Tradeoff"]},{"cell_type":"markdown","metadata":{"id":"6_r4vIiX7gzn","colab_type":"text"},"source":["* **K** is called a *tuning parameter*\n","\n","* ***Variance issue (overfitting)***: **k** is to *small* so there is too much variance between sets of **k** data points \n","\n","  * eg. We could skew our mean in the wrong direction because we don’t have enough data points with (close to) 28 degrees\n","\n","* ***Bias issue (underfitting)***: **k** is to *large* so there is to big of a gap from the features we are using to predict to some of the points\n","\n","  * eg. to far from 28 degrees to get a good estimate\n","\n","* So we just need to find a *happy medium* :)\n","\n","  * BTW, often a larger dataset allows us to increase **k** ..."]},{"cell_type":"markdown","metadata":{"id":"SjJa4O-RRs1R","colab_type":"text"},"source":["\n","### 1.7.2. Relation to Overall Dataset, 1.7.3. Well Then, What Is the Best Value of k?"]},{"cell_type":"markdown","metadata":{"id":"sQ26NjnhVVKl","colab_type":"text"},"source":["* say our data has ***n*** rows\n","* the larger ***n*** is, the larger we can make **k** \n","* eg. if we had 2000 rows, the 25 NN may all be close to 28 degrees"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3w-a2-Q37SE5","colab":{}},"source":["# check the number of rows and columns\n","print(day1.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_NteUNnqt-d","colab_type":"text"},"source":["* there are lots o different proposed methods of choosing **k** but no one method is perfect or works in all situations\n","\n","  * **k < sqrt(n)** is one way \n","\n","* we will talk about another one (the *leave one out* method) later in the chapter"]},{"cell_type":"markdown","metadata":{"id":"nUtPTtWBtGgu","colab_type":"text"},"source":["### 1.7.4. What about the # of Predictors?"]},{"cell_type":"markdown","metadata":{"id":"5RgJmuD-tLr_","colab_type":"text"},"source":["* similarily, the number of peredictors/features (p) needs to be appropriate for our data size\n","\n","* say zip code is our feature and we only have 10k rows to predict weather, then we would have only about 2 rows per zip code\n","  * we would *bias* our data in the direction of whatever the 2 rows say\n","\n","* one rule of thumb is **p < sqrt(n)** again"]},{"cell_type":"markdown","metadata":{"id":"oMWWplU1plUu","colab_type":"text"},"source":["# 1.8. The Regression Function: What Is Being “Learned” in Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"njH48qDPpx1K","colab_type":"text"},"source":["* The regression function here is also known as the *conditional mean* because we are inputting the condition \n","\n","  * eg. **r(28)** in the previous example\n","\n","  * eg. If we also had *humidity* = **0.51** then it would be **r(28, 0.51)**\n","\n","* we estimate the relationship between input/output using these **features/predictors** along with their **outcomes** as training set \n","\n","  * ie. we learn the regression function **r( )**\n"]},{"cell_type":"markdown","metadata":{"id":"cC5BLwsH0uR6","colab_type":"text"},"source":["## 1.8.2. *Exact* Mean?"]},{"cell_type":"markdown","metadata":{"id":"2Ul7UlSjqtUD","colab_type":"text"},"source":["* *actually*, we are **estimating** based on a *sample* of the *population*\n","\n","  * eg. a survey can only taken by people who opt to do it\n","\n","    * This is why there is always a **margin of error** associated with each prediction\n","\n","  * also **k** is a sample\n","\n","\n","* Smaller samples are less valuable\n","\n","  * Eg. may be no other temperatures with 28 degrees in small dataset\n","\n","\n","* in *any* ML problem we need to try to figure out the **best** method to use given our sample size and features available"]},{"cell_type":"markdown","metadata":{"id":"qoZU9EkrdhYe","colab_type":"text"},"source":["# 1.9. Informal Notation: the “X” and the “Y”"]},{"cell_type":"markdown","metadata":{"id":"2raewDZO7wMU","colab_type":"text"},"source":["* Typically, features are called **X**\n","\n","  * with ***p*** predictors\n","\n","* while **Y** is the outcome to be predicted (for shorthand)\n","\n","  * In categorical multi-class problems **Y** could be a factor or multiple dummy columns"]},{"cell_type":"markdown","metadata":{"id":"g_sBzaYgrBy-","colab_type":"text"},"source":["# 1.10. So, Let’s Do Some Analysis, The Bike Sharing Data"]},{"cell_type":"markdown","metadata":{"id":"ZVeaow4yrYuQ","colab_type":"text"},"source":["* Lets predict **ridership** using only **working day, numeric, and weather variable**s using **kNN** (basic)"]},{"cell_type":"code","metadata":{"id":"ZmHoG4XbUf1v","colab_type":"code","colab":{}},"source":["# extract X columns we want to use by name using .loc\n","day1 = day1.loc[:,['workingday','temp','atemp','hum', 'windspeed','tot']]\n","day1.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FukXLlIdrq_W","colab_type":"text"},"source":["* the **X** (predictor columns) are just the first 5 columns:"]},{"cell_type":"code","metadata":{"id":"5MW59hvFcRqi","colab_type":"code","colab":{}},"source":["# and use iloc to extract columns by number\n","day1x = day1.iloc[:,0:5] \n","day1x.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcXPCWvXtUO2","colab_type":"text"},"source":["* while the **Y** is the **tot** column (6th column):\n"]},{"cell_type":"code","metadata":{"id":"it6xOg-ZtW4H","colab_type":"code","outputId":"59d7054b-e2f0-48e7-f60c-f83937e5b1d4","executionInfo":{"status":"ok","timestamp":1587790205179,"user_tz":420,"elapsed":353,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# while the Y is the tot column (6th column)\n","tot = day1.tot\n","tot.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     985\n","1     801\n","2    1349\n","3    1562\n","4    1600\n","Name: tot, dtype: int64"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"v7ye51VjtsSZ","colab_type":"text"},"source":["* we use **classifier** for predicting classes in chapter 2, but here in chapter 1 we are starting with **regression**:"]},{"cell_type":"code","metadata":{"id":"YHP4sn1PJKKQ","colab_type":"code","colab":{}},"source":["# from sklearn.neighbors import KNeighborsClassifier \n","from sklearn.neighbors import KNeighborsRegressor"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIUqmDYjihf0","colab_type":"text"},"source":["## 1.10.3. Distances and 1.10.4. Scaling"]},{"cell_type":"markdown","metadata":{"id":"VcIdO-m2i25v","colab_type":"text"},"source":["* we take the square root of each squared distance for each feature in the data\n","\n","  * Must square them so positive and negative distances don’t cancel out\n","\n","  * eg. `height_1 - height_2` and `weight_1 - weight_2` **in mlb data (p.17)**\n","\n","* But, if there is a large range between the average feature values (eg. height greater in this) we would be placing higher priority it, in the distance calculation\n","\n","  * ***scaling*** can be used to balance out the weight of each distance\n","\n","  * for **standard scaling**, we divide each by its standard deviation giving each a standard deviation of 1; subtract mean to give mean 0\n","\n","    * In my kNN (shown in 1.11.) this kind is done by default, but we can set **scaleX=F** "]},{"cell_type":"markdown","metadata":{"id":"LiwoB9_Bt3O-","colab_type":"text"},"source":["* these are some types of scaling we can do with **sklrean**"]},{"cell_type":"code","metadata":{"id":"B1ULiquQtyv7","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import RobustScaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NFi1-URcmWdL","colab_type":"text"},"source":["* **Robust Scaling:** this method is commonly used to overcome the presence of outliers in the data. \n","\n","  * This version uses the **median** and **quartile range**\n","    * x_i' = (x_i-Q1)/(Q3-Q1) \n","\n","* lets try starting with this to compare to **standard** later"]},{"cell_type":"code","metadata":{"id":"Asd0T6KY0MAS","colab_type":"code","colab":{}},"source":["# set the scaler\n","scaler = RobustScaler()\n","\n","# scale columns:\n","columns_d1x = [list(day1x.columns)]\n","columns_d1x\n","for feature in columns_d1x:\n","    day1x[feature] = scaler.fit_transform(day1x[feature])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wthl5SGEksY-","colab_type":"text"},"source":["* lets try 5 nearest neighbors"]},{"cell_type":"code","metadata":{"id":"YG1Sd1tvmh3I","colab_type":"code","colab":{}},"source":["# lets try 5 nearest neighbors:\n","knn = KNeighborsRegressor(n_neighbors=5)\n","\n","# fit the model (set our X,Y to base prediction on)\n","knn.fit(day1x, tot) \n","\n","# our test data also needs to be 2d (double brackets):\n","test = [[1,12.0,11.8,0.23,5]] \n","\n","# Predict on data which model has not seen before:\n","print(knn.predict(test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4ZskbhcWizi","colab_type":"code","colab":{}},"source":["# check the indices of nearest neighbors and their values:\n","nn = knn.kneighbors(test)[1][0]\n","print(day1.loc[nn,:])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mOlPGFmgxPq","colab_type":"code","colab":{}},"source":["from statistics import mean\n","print(mean(list(tot.loc[nn])))\n","# prediction above matches:"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gq6tq9X_0GZZ","colab_type":"code","colab":{}},"source":["# lets check the data types:\n","day1x.dtypes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmZ7yvaPlp4h","colab_type":"text"},"source":["* lets try both workingday and non-workingday cases"]},{"cell_type":"code","metadata":{"id":"DDgJRTkGj8RZ","colab_type":"code","colab":{}},"source":["newx = [[1,12.0,11.8,0.23,5],[0,12.0,11.8,0.23,5]]\n","# also works:\n","# newx1 = [1,12.0,11.8,0.23,5]\n","# newx2 = [0,12.0,11.8,0.23,5]\n","# newx = pd.DataFrame([newx1,newx2])#, columns = day1x.columns)\n","\n","# still using k=5\n","print(knn.predict(newx)) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kr2aQCMSpV5R","colab_type":"text"},"source":["# 1.11. Choosing Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"tta06r23myig","colab_type":"text"},"source":["* One way of choosing **hyperparameters** is to test predicting the original data with various input values\n","\n","  * eg. In the **kNN** example we would try different values of **k**"]},{"cell_type":"markdown","metadata":{"id":"hxdyWvafl7q7","colab_type":"text"},"source":["* lets try my *Python* version of **Matloff's Regtools kNN function** (from the textbook)\n","\n","* **kNN** returns variables called **all_preds** and **nn**...\n","\n","* so, if **allK=True**, each *column* of **all_preds** is a predicted value for the corresponding row of the test data \n","\n","* while each *row* of **ypreds** uses a different value of k for the Y prediction for that data point\n"," \n","* **nn** is an array for the **k** closest data points (indices) for each row of the test data"]},{"cell_type":"code","metadata":{"id":"tkdaHeFhDUzl","colab_type":"code","colab":{}},"source":["# my function to implement kNN (with allK, leave1out options) from Matloff's R Regtools package:\n","\n","# note, this is sklearn knn updated to be used for ch1 and 2 of the text, any other features included in\n","# regtools are knn not necessary here\n","\n","def kNN(X,Y,newx,k,regress=True,allK=False,leave1out=False,scaleX=True,scaler=StandardScaler()):\n","\n","  import warnings\n","  warnings.filterwarnings('ignore')\n","\n","  import numpy as np\n","\n","  from sklearn.neighbors import KNeighborsClassifier \n","  from sklearn.neighbors import KNeighborsRegressor\n","\n","  from sklearn.preprocessing import StandardScaler\n","  from sklearn.preprocessing import MinMaxScaler\n","  from sklearn.preprocessing import RobustScaler\n","\n","  from statistics import mean \n","  from statistics import mode\n","  from collections import Counter\n","\n","  def kNNtype(neighbs,regress):\n","    if regress:\n","      knn = KNeighborsRegressor(n_neighbors=neighbs)\n","    else:\n","      knn = KNeighborsClassifier(n_neighbors=neighbs)\n","    return knn\n","\n","  if scaler != StandardScaler():\n","    scaler=scaler\n","\n","\n","# Update: for row subsets/test sets in hw; fitting the scaling function should be done separate,\n","# so the same can be applied to train and test data (or X and newx) \n","  if scaleX == True:\n","    # scale should be fit to X/train\n","    scaler.fit(X)\n","    newx = pd.DataFrame(scaler.transform(newx))\n","    X = pd.DataFrame(scaler.transform(X))\n","  \n","\n","  knn_all = pd.DataFrame()\n","  if allK == True:\n","    if leave1out == True:\n","      nn_all = []\n","      for j in list(newx.index.values.tolist()): #(4/13)\n","      #for j in list(Y.index.values.tolist()): \n","        knn_row = []\n","        knn = kNNtype(k+1,regress)\n","        knn.fit(X, Y)\n","        test = pd.DataFrame(newx.loc[j,:])\n","        nn = knn.kneighbors(test.T)[1][0]\n","        for i in range(2,k+1):\n","          nn1 = nn[1:i] # leave one out\n","          test = list(Y.iloc[nn1])\n","          if regress:\n","            test = mean(test)\n","          else:\n","            c = Counter(test)\n","            l = list(c.values())\n","            ind = l.index(max(c.values()))\n","            test = list(c.keys())[ind]\n","            # count number of times the max class occurs and if there is a tie\n","            # choose the second class with the max if index is even\n","            if (l.count(max(l))) > 1 and (j % 2 !=0):\n","              l[ind] = 0\n","              ind = l.index(max(c.values()))\n","              test = list(c.keys())[ind2]\n","\n","          knn_row.append(test)\n","        knn_row = pd.DataFrame(knn_row)\n","        knn_all = [knn_all, knn_row]\n","        knn_all = pd.concat(knn_all,axis=1, ignore_index=True)\n","        nn_all.append(list(nn1))\n","      nn_all = np.array(nn_all)\n","    else:\n","        for i in range(1,k+1):\n","          knn = kNNtype(i,regress)\n","          knn.fit(X, Y)\n","          test = knn.predict(newx)\n","          knn_row = pd.DataFrame(test).T\n","          knn_all = [knn_all, knn_row]\n","          knn_all = pd.concat(knn_all,axis=0, ignore_index=True)\n","        nn_all = knn.kneighbors(newx)[1]\n","  else:\n","    if leave1out == True:\n","      knn_row = []\n","      #for j in list(Y.index.values.tolist()):\n","      for j in list(newx.index.values.tolist()): # (4/13)\n","        knn = kNNtype(k,regress)\n","        knn.fit(X, Y)\n","        test = pd.DataFrame(newx.loc[j,:])\n","        nn = knn.kneighbors(test.T)[1][0]\n","        nn1 = nn[1:len(nn)]\n","\n","        test = list(Y.iloc[nn1])\n","        if regress:\n","            test = mean(test)\n","        else:\n","          c = Counter(test)\n","          l = list(c.values())\n","          ind = l.index(max(c.values()))\n","          test = list(c.keys())[ind]\n","          # count number of times the max class occurs and if there is a tie\n","          # choose the second class with the max if index is even\n","          if (l.count(max(l))) > 1 and (j % 2 !=0):\n","            l[ind] = 0\n","            ind = l.index(max(c.values()))\n","            test = list(c.keys())[ind2]\n","\n","        knn_row.append(test)\n","      knn_all = pd.DataFrame(knn_row).T\n","      nn_all = nn1\n","    else:\n","        knn = kNNtype(k,regress)\n","        knn.fit(X, Y)\n","        test = knn.predict(newx)\n","        knn_all = pd.DataFrame(test)\n","        nn_all = knn.kneighbors(newx)[1]\n","\n","  return knn_all, nn_all"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qiuYVFYMYN_E","colab_type":"code","outputId":"0dff24a9-3a97-4548-d576-df06fb631ab2","executionInfo":{"status":"ok","timestamp":1587790163309,"user_tz":420,"elapsed":352,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# extract X columns we want to use by name using .loc\n","day1 = day1.loc[:,['workingday','temp','atemp','hum', 'windspeed','tot']]\n","day1.head()\n","# and use iloc to extract columns by number\n","day1x = day1.iloc[:,0:5] \n","day1x.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>workingday</th>\n","      <th>temp</th>\n","      <th>atemp</th>\n","      <th>hum</th>\n","      <th>windspeed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>8.175849</td>\n","      <td>7.999250</td>\n","      <td>0.805833</td>\n","      <td>10.749882</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>9.083466</td>\n","      <td>7.346774</td>\n","      <td>0.696087</td>\n","      <td>16.652113</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1.229108</td>\n","      <td>-3.499270</td>\n","      <td>0.437273</td>\n","      <td>16.636703</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1.400000</td>\n","      <td>-1.999948</td>\n","      <td>0.590435</td>\n","      <td>10.739832</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>2.666979</td>\n","      <td>-0.868180</td>\n","      <td>0.436957</td>\n","      <td>12.522300</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   workingday      temp     atemp       hum  windspeed\n","0           0  8.175849  7.999250  0.805833  10.749882\n","1           0  9.083466  7.346774  0.696087  16.652113\n","2           1  1.229108 -3.499270  0.437273  16.636703\n","3           1  1.400000 -1.999948  0.590435  10.739832\n","4           1  2.666979 -0.868180  0.436957  12.522300"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"Uq4eTUsxMb9c","colab_type":"text"},"source":["* lets try it for the 8 nearest neighbors!"]},{"cell_type":"code","metadata":{"id":"4-MksjIZsWZV","colab_type":"code","outputId":"08401665-2c0e-4ebc-b6de-fa6c65f4270b","executionInfo":{"status":"ok","timestamp":1587790216504,"user_tz":420,"elapsed":367,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["all_preds, nn = kNN(day1x,tot,day1x[0:3],8,allK=True)\n","print(all_preds)\n","print(nn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["             0            1            2\n","0   985.000000   801.000000  1349.000000\n","1  2817.000000  2735.000000  1449.500000\n","2  2696.333333  3901.666667  1479.000000\n","3  2746.000000  3445.500000  1932.250000\n","4  2399.400000  3215.200000  1810.000000\n","5  2537.500000  3165.666667  2035.500000\n","6  2591.857143  3160.000000  2220.857143\n","7  2741.125000  3449.750000  2219.500000\n","[[  0 701  98  99 724 708 274 715]\n"," [  1 687 455  63 365 274 104 646]\n"," [  2  33  40 383   9 384 429  83]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NZ4ZyfIwXho8","colab_type":"code","outputId":"6855e321-ac74-407d-bd52-a4eecdf298e4","executionInfo":{"status":"ok","timestamp":1587790218457,"user_tz":420,"elapsed":619,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["all_preds, nn = kNN(day1x,tot,day1x,8,allK=True)\n","print(all_preds)\n","print(nn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["           0            1            2    ...          728          729          730\n","0   985.000000   801.000000  1349.000000  ...  1341.000000  1796.000000  2729.000000\n","1  2817.000000  2735.000000  1449.500000  ...  2086.500000  2036.500000  2145.500000\n","2  2696.333333  3901.666667  1479.000000  ...  1642.333333  2008.000000  2462.000000\n","3  2746.000000  3445.500000  1932.250000  ...  1485.000000  1943.250000  2273.500000\n","4  2399.400000  3215.200000  1810.000000  ...  1407.200000  2228.200000  2215.800000\n","5  2537.500000  3165.666667  2035.500000  ...  1663.833333  2427.333333  2096.666667\n","6  2591.857143  3160.000000  2220.857143  ...  1583.000000  2198.000000  2143.285714\n","7  2741.125000  3449.750000  2219.500000  ...  1820.750000  2061.625000  2586.125000\n","\n","[8 rows x 731 columns]\n","[[  0 701  98 ... 708 274 715]\n"," [  1 687 455 ... 274 104 646]\n"," [  2  33  40 ... 384 429  83]\n"," ...\n"," [728 399 358 ... 400  28 337]\n"," [729 693 366 ... 428   8  51]\n"," [730   3 727 ...  30 362 675]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iDBu3OTAMZot","colab_type":"text"},"source":["* eg. get indices of the 8 closest neighbors of the 1st data point (index 0):"]},{"cell_type":"code","metadata":{"id":"RzutcCrwlXR0","colab_type":"code","outputId":"01fcdada-e329-4b0c-885d-5404f6b337d5","executionInfo":{"status":"ok","timestamp":1587790220389,"user_tz":420,"elapsed":309,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(nn[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[  0 701  98  99 724 708 274 715]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4_qcVF7JNM6P","colab_type":"text"},"source":["* of course the closest index to 0 is 0...\n","\n","* lets look at some of the actual y values (the first 5)\n","\n","* notice that when k = 1 (row 0 of ypreds above), its exactly correct since the test set is the same one we used to fit the function, so it just uses the point itself to predict itself"]},{"cell_type":"code","metadata":{"id":"94Y5sBITDi_b","colab_type":"code","colab":{}},"source":["print(tot[0:5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sk2646oTONtk","colab_type":"text"},"source":["* how do we fix this!?\n","\n","* using kNN's option **leave1out = True** helps us better replicate an actual prediction using k nearest neighbors **by excluding its own features as neighbors (not using the 1st nearest neighbor):**"]},{"cell_type":"code","metadata":{"id":"X3nD2UnBmPEE","colab_type":"code","outputId":"08e6c592-19c5-477d-d581-e775347feab8","executionInfo":{"status":"ok","timestamp":1587790236248,"user_tz":420,"elapsed":4050,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["knnout, nn = kNN(day1x,tot,day1x,8,allK = True,leave1out = True) \n","\n","print(knnout)\n","print(nn)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["           0            1            2    ...          728          729          730\n","0  4649.000000  4669.000000  1550.000000  ...  2832.000000  2277.000000  1562.000000\n","1  3552.000000  5452.000000  1544.000000  ...  1793.000000  2114.000000  2328.500000\n","2  3333.000000  4327.000000  2126.666667  ...  1533.000000  1992.333333  2121.666667\n","3  2753.000000  3818.750000  1925.250000  ...  1423.750000  2336.250000  2087.500000\n","4  2848.000000  3638.600000  2172.800000  ...  1728.400000  2553.600000  1970.200000\n","5  2859.666667  3553.166667  2366.166667  ...  1623.333333  2265.000000  2045.666667\n","6  2992.000000  3828.142857  2343.857143  ...  1889.285714  2099.571429  2565.714286\n","\n","[7 rows x 731 columns]\n","[[701  98  99 ... 708 274 715]\n"," [687 455  63 ... 274 104 646]\n"," [ 33  40 383 ... 384 429  83]\n"," ...\n"," [399 358 724 ... 400  28 337]\n"," [693 366 721 ... 428   8  51]\n"," [  3 727  34 ...  30 362 675]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jHnUGygOpLpf","colab_type":"text"},"source":["## 1.11.2. Evaluating Prediciton Accuracy"]},{"cell_type":"markdown","metadata":{"id":"Grhw3b_lOxa-","colab_type":"text"},"source":["* you can also use my function to do **Matloff's Regtools findOverallLoss()** described in the textbook"]},{"cell_type":"code","metadata":{"id":"1SDjQJxMGPMb","colab_type":"code","colab":{}},"source":["def findOverallLoss(ypreds,Y):\n","  err = abs(ypreds.reset_index(drop=True) - Y.reset_index(drop=True).values.squeeze())\n","  MAPE = err.mean(axis=1)\n","  return MAPE\n","# this function finds the prediction accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xOuO1vtQeeR","colab_type":"text"},"source":["* the findOverallLoss function can be used to compare\n","\n","* the distance between a predicted value and its actual value\n","\n","* by calculating the MAPE (Mean Absolute Prediciton Error) for each value of k"]},{"cell_type":"code","metadata":{"id":"BcynUzU3L9ai","colab_type":"code","outputId":"73e10ebb-6a7f-4998-aace-3ad9c358b530","executionInfo":{"status":"ok","timestamp":1587790306898,"user_tz":420,"elapsed":502,"user":{"displayName":"Robin Yancey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1Mgq9pKcR0qfYMWBX5nKU1OYkU75h8XMWnSc=s64","userId":"13168997243252415066"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["findOverallLoss(knnout,tot)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    1362.696306\n","1    1181.111491\n","2    1121.279526\n","3    1071.652873\n","4    1085.303420\n","5    1069.830141\n","6    1060.946453\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"c9w_RCvRRilK","colab_type":"text"},"source":["* **can we do better?:**\n","\n","* since 1000 is a significant # of riders compared to\n","* the average number of riders:\n","* we might look for other ways to imrove"]},{"cell_type":"code","metadata":{"id":"3SZKLFFAQrYs","colab_type":"code","colab":{}},"source":["# eg.\n","mean(tot) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQlPyIOcR1_u","colab_type":"text"},"source":["* for example, we can look at other ways of using the timing data..,\n","* this plot show the total number of riders as time goes on because the rows are already in chronological order"]},{"cell_type":"code","metadata":{"id":"SMhmXeWXFY5y","colab_type":"code","colab":{}},"source":["tot.plot(kind='line')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDvz0ptFSHdH","colab_type":"text"},"source":["* what can we tell from this?\n","\n","* we can see that there is a seasonal trend and upward trend \n","* so lets 1. make row number a feature and 2. increase k:"]},{"cell_type":"code","metadata":{"id":"Z_KLm4n0F_YU","colab_type":"code","colab":{}},"source":["# make row number feature:\n","dayNum=pd.DataFrame(range(0,731),columns=['dayNum'])\n","test = pd.concat([day1,dayNum],axis=1)\n","day1x = test.iloc[:,[0,1,2,3,4,6]]\n","# this is how we could change the data types if needed:\n","# day1x.astype('float64').dtypes\n","# day1x.dtypes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMqVbAPjiW_f","colab_type":"code","colab":{}},"source":["# predict again but with k=25\n","preds, nn = kNN(day1x,tot,day1x,25,allK=True,leave1out=True)\n","print(preds, nn)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecbX15HhU4gx","colab_type":"code","colab":{}},"source":["# check new loss\n","losses = findOverallLoss(preds,tot)\n","print(losses)\n","\n","list(losses.index[losses == min(losses)])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydg1nFkRTNLN","colab_type":"text"},"source":["* looks like 11 is the best k (because it has the lowest loss)\n","* and its has a much lower value (than using k=8 and no row numbers above), yay!"]},{"cell_type":"markdown","metadata":{"id":"y_1cxVonp5aU","colab_type":"text"},"source":["# 1.13. Going Further with This Data"]},{"cell_type":"markdown","metadata":{"id":"cqIx3Uj_VL2x","colab_type":"text"},"source":["* looks like summer is also a good indicator of higher ridership\n","* lets try adding it in:\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1rzaEpxCqqIm","colab":{}},"source":["# season was removed so reload original data:\n","day1 = pd.read_csv(my_path)\n","\n","# make a column of zeros first\n","summer = np.zeros(shape=(1,day1.shape[0])).T\n","summer = pd.DataFrame(summer,columns=['summer'])\n","\n","# find indices with summer (season == 3)\n","inds = day1.index[day1['season'] == 3].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7IxZnDgoJ7C","colab_type":"code","colab":{}},"source":["# set the summer variable to 1 everywhere season is summer\n","# eg. indicator for summer\n","summer.iloc[inds,] = 1\n","test = pd.concat([day1x,summer],axis=1)\n","test.head()\n","\n","# we could also change columns to categorys/factors\n","# test['workingday'] = test['workingday'].astype('category')\n","# test['summer'] = test['summer'].astype('category')\n","# test.dtypes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWV_1cDSauzI","colab_type":"text"},"source":["* looks like the minimum loss is still at k=6, and the loss is actually very slightly higher (unlike Dr. Matloff thought)... hm?\n","\n","* Robin will double check this later :)"]},{"cell_type":"code","metadata":{"id":"XP8r2eypvLyQ","colab_type":"code","colab":{}},"source":["preds, nn = kNN(test,tot,test,25,allK=True,leave1out=True) \n","\n","losses = findOverallLoss(preds,tot)\n","print(losses)\n","\n","list(losses.index[losses == min(losses)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YGVlsShIo0CN","colab_type":"text"},"source":["# 1.15. p-Hacking"]},{"cell_type":"markdown","metadata":{"id":"ipyaiV-IqMJi","colab_type":"text"},"source":["* Say we toss 250 coins 100 times and say that >40 or <60 heads denotes an unbalanced coin\n","\n","  * eg. An balanced coin has a 5% chance of being denoted unbalanced \n","    * eg. 95% is within 2 standard deviations of the mean\n","\n","      * st.dev= sqrt(var)=npq=sqrt(100*1/2*1/2)=5\n","\n","* By chance, 5% of 250 (12.5) will be wrongly declared unbalanced\n","\n","* P-hacking means that when so many features are considered at least one is likely to be seen as *statistically significant*\n","\n","  * eg. In a ML data contest there so many competitors each with there own methods - at least one is probably going to emerge as superior (due to random variation)\n","\n","    * It may be just by accident and not be the winner in the next competition on the same type of data (and other data points)\n","\n","  * Lets say we have 4 tuning parameters with 10 possible values- thats 10,000 combinations!"]},{"cell_type":"markdown","metadata":{"id":"1Ec1a4slrCef","colab_type":"text"},"source":["# 1.16. Pitfall: Dirty Data"]},{"cell_type":"markdown","metadata":{"id":"3vsrbyCQrA4C","colab_type":"text"},"source":["* as indicated in the previous lecture, NA removal  could lead to either\n","\n","  * throwing out a lot of rows of data (eg. too much) that could be helpful for better prediction\n","\n","    * when we could just fill in those columns so we can still use the rows\n","\n","  * or leaving out important information that only can be obtained from those rows with NA\n","\n","      * eg. those that decline to respond may have other features different than the rest of the data"]}]}